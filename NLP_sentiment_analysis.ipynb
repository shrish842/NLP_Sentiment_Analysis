{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade fsspec"
      ],
      "metadata": {
        "id": "KhMd2K7QvK0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqwpKLW-uFdk"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets torch scikit-learn pandas -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score,f1_score\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer"
      ],
      "metadata": {
        "id": "p51MaSVAubUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imdb_dataset = load_dataset(\"imdb\")\n",
        "print(imdb_dataset)\n",
        "print(\"\\nSample Training Example:\")\n",
        "print(imdb_dataset['train'][0])\n",
        "print(\"\\nSample Testing Example:\")\n",
        "print(imdb_dataset['test'][0])\n",
        "\n",
        "# Check label distribution\n",
        "train_df = pd.DataFrame(imdb_dataset['train'])\n",
        "test_df = pd.DataFrame(imdb_dataset['test'])\n",
        "print(\"\\nTraining label distribution:\")\n",
        "print(train_df['label'].value_counts())\n",
        "# Label 0 is typically negative, Label 1 is positive"
      ],
      "metadata": {
        "id": "ZvOGgFrswDqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the training set into train and validation (e.g., 90% train, 10% validation)\n",
        "train_val_split = imdb_dataset['train'].train_test_split(test_size=0.1, seed=42) # Use a seed for reproducibility\n",
        "\n",
        "train_dataset = train_val_split['train']\n",
        "val_dataset = train_val_split['test']\n",
        "test_dataset = imdb_dataset['test']\n",
        "\n",
        "print(\"\\nDataset splits:\")\n",
        "print(f\"Training examples: {len(train_dataset)}\")\n",
        "print(f\"Validation examples: {len(val_dataset)}\")\n",
        "print(f\"Test examples: {len(test_dataset)}\")"
      ],
      "metadata": {
        "id": "rpFjqqPwwjRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = \"distilbert-base-uncased\""
      ],
      "metadata": {
        "id": "yohwqOxWywnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "mEpf_vVzy44D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"This is a test sentence for the tokenizer.\"\n",
        "encoded_input = tokenizer(sample_text)\n",
        "print(\"\\nTokenized Sample:\")\n",
        "print(encoded_input)\n",
        "print(\"Decoded tokens:\", tokenizer.convert_ids_to_tokens(encoded_input['input_ids']))"
      ],
      "metadata": {
        "id": "nn8_dC9Ny-ai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_labels = 2 # Positive or Negative\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)\n",
        "\n",
        "# Check if GPU is available and move model to GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(f\"\\nModel loaded on device: {device}\")"
      ],
      "metadata": {
        "id": "mzZr4SqdzC-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    # Tokenize the text. The tokenizer handles padding and truncation.\n",
        "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
        "\n",
        "# Apply the tokenization function to all splits of the dataset\n",
        "# Use batched=True for faster processing\n",
        "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Remove the original 'text' column as it's no longer needed\n",
        "# Keep 'input_ids', 'attention_mask', 'label'\n",
        "tokenized_train_dataset = tokenized_train_dataset.remove_columns([\"text\"])\n",
        "tokenized_val_dataset = tokenized_val_dataset.remove_columns([\"text\"])\n",
        "tokenized_test_dataset = tokenized_test_dataset.remove_columns([\"text\"])\n",
        "\n",
        "# Rename the 'label' column to 'labels' (expected by the Trainer)\n",
        "tokenized_train_dataset = tokenized_train_dataset.rename_column(\"label\", \"labels\")\n",
        "tokenized_val_dataset = tokenized_val_dataset.rename_column(\"label\", \"labels\")\n",
        "tokenized_test_dataset = tokenized_test_dataset.rename_column(\"label\", \"labels\")\n",
        "\n",
        "# Set the format to PyTorch tensors\n",
        "tokenized_train_dataset.set_format(\"torch\")\n",
        "tokenized_val_dataset.set_format(\"torch\")\n",
        "tokenized_test_dataset.set_format(\"torch\")\n",
        "\n",
        "print(\"\\nProcessed dataset sample (train):\")\n",
        "print(tokenized_train_dataset[0])"
      ],
      "metadata": {
        "id": "7nhLaqrXzR0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1) # Get the index of the highest logit\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    f1 = f1_score(labels, predictions, average='binary') # Use 'weighted' for multi-class\n",
        "    return {\"accuracy\": accuracy, \"f1\": f1}"
      ],
      "metadata": {
        "id": "PBiWay5rzcgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the directory where model checkpoints will be saved\n",
        "output_dir = \"./sentiment_model_results\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,                   # Directory to save model checkpoints\n",
        "    num_train_epochs=3,                      # Total number of training epochs (start with 1-3)\n",
        "    per_device_train_batch_size=16,          # Batch size per device during training\n",
        "    per_device_eval_batch_size=32,           # Batch size for evaluation\n",
        "    warmup_steps=500,                        # Number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,                       # Strength of weight decay regularization\n",
        "    logging_dir='./logs',                    # Directory for storing logs\n",
        "    logging_steps=100,                       # Log metrics every N steps\n",
        "    evaluation_strategy=\"epoch\",             # Evaluate performance at the end of each epoch\n",
        "    save_strategy=\"epoch\",                   # Save a model checkpoint at the end of each epoch\n",
        "    load_best_model_at_end=True,             # Load the best model (based on validation metric) at the end\n",
        "    metric_for_best_model=\"f1\",              # Metric to determine the best model (can be accuracy, f1, etc.)\n",
        "    greater_is_better=True,                  # True if a higher metric value is better\n",
        "    fp16=torch.cuda.is_available(),          # Use mixed precision training if GPU is available (faster, less memory)\n",
        "    report_to=\"none\"                         # Disable reporting to external services like W&B for this example\n",
        ")"
      ],
      "metadata": {
        "id": "46rg8D_dz72Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,                             # The instantiated Transformers model to be trained\n",
        "    args=training_args,                      # Training arguments defined above\n",
        "    train_dataset=tokenized_train_dataset,   # Training dataset\n",
        "    eval_dataset=tokenized_val_dataset,      # Evaluation dataset\n",
        "    tokenizer=tokenizer,                     # Tokenizer (needed for padding collation)\n",
        "    compute_metrics=compute_metrics          # Function to compute evaluation metrics\n",
        ")"
      ],
      "metadata": {
        "id": "mZKGg1NN0KGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nStarting training...\")\n",
        "train_result = trainer.train()\n",
        "print(\"\\nTraining finished.\")\n",
        "\n",
        "# You can print some training stats\n",
        "print(f\"Training Metrics: {train_result.metrics}\")"
      ],
      "metadata": {
        "id": "qCKpTki20PBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nEvaluating on the test set...\")\n",
        "eval_results = trainer.evaluate(eval_dataset=tokenized_test_dataset)\n",
        "\n",
        "print(\"\\nTest Set Evaluation Results:\")\n",
        "print(eval_results)\n",
        "# Example output: {'eval_loss': 0.XXXX, 'eval_accuracy': 0.YYYY, 'eval_f1': 0.ZZZZ, ...}"
      ],
      "metadata": {
        "id": "rhd-3LT70Sjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "woQ-znWM36l5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}