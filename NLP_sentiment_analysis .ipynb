{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade fsspec"
      ],
      "metadata": {
        "id": "KhMd2K7QvK0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqwpKLW-uFdk"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets torch scikit-learn pandas -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score,f1_score\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer"
      ],
      "metadata": {
        "id": "p51MaSVAubUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imdb_dataset = load_dataset(\"imdb\")\n",
        "print(imdb_dataset)\n",
        "print(\"\\nSample Training Example:\")\n",
        "print(imdb_dataset['train'][0])\n",
        "print(\"\\nSample Testing Example:\")\n",
        "print(imdb_dataset['test'][0])\n",
        "\n",
        "# Check label distribution\n",
        "train_df = pd.DataFrame(imdb_dataset['train'])\n",
        "test_df = pd.DataFrame(imdb_dataset['test'])\n",
        "print(\"\\nTraining label distribution:\")\n",
        "print(train_df['label'].value_counts())\n",
        "# Label 0 is typically negative, Label 1 is positive"
      ],
      "metadata": {
        "id": "ZvOGgFrswDqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the training set into train and validation (e.g., 90% train, 10% validation)\n",
        "train_val_split = imdb_dataset['train'].train_test_split(test_size=0.1, seed=42) # Use a seed for reproducibility\n",
        "\n",
        "train_dataset = train_val_split['train']\n",
        "val_dataset = train_val_split['test']\n",
        "test_dataset = imdb_dataset['test']\n",
        "\n",
        "print(\"\\nDataset splits:\")\n",
        "print(f\"Training examples: {len(train_dataset)}\")\n",
        "print(f\"Validation examples: {len(val_dataset)}\")\n",
        "print(f\"Test examples: {len(test_dataset)}\")"
      ],
      "metadata": {
        "id": "rpFjqqPwwjRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = \"distilbert-base-uncased\""
      ],
      "metadata": {
        "id": "yohwqOxWywnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "mEpf_vVzy44D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"This is a test sentence for the tokenizer.\"\n",
        "encoded_input = tokenizer(sample_text)\n",
        "print(\"\\nTokenized Sample:\")\n",
        "print(encoded_input)\n",
        "print(\"Decoded tokens:\", tokenizer.convert_ids_to_tokens(encoded_input['input_ids']))"
      ],
      "metadata": {
        "id": "nn8_dC9Ny-ai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_labels = 2 # Positive or Negative\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)\n",
        "\n",
        "# Check if GPU is available and move model to GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(f\"\\nModel loaded on device: {device}\")"
      ],
      "metadata": {
        "id": "mzZr4SqdzC-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    # Tokenize the text. The tokenizer handles padding and truncation.\n",
        "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
        "\n",
        "# Apply the tokenization function to all splits of the dataset\n",
        "# Use batched=True for faster processing\n",
        "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Remove the original 'text' column as it's no longer needed\n",
        "# Keep 'input_ids', 'attention_mask', 'label'\n",
        "tokenized_train_dataset = tokenized_train_dataset.remove_columns([\"text\"])\n",
        "tokenized_val_dataset = tokenized_val_dataset.remove_columns([\"text\"])\n",
        "tokenized_test_dataset = tokenized_test_dataset.remove_columns([\"text\"])\n",
        "\n",
        "# Rename the 'label' column to 'labels' (expected by the Trainer)\n",
        "tokenized_train_dataset = tokenized_train_dataset.rename_column(\"label\", \"labels\")\n",
        "tokenized_val_dataset = tokenized_val_dataset.rename_column(\"label\", \"labels\")\n",
        "tokenized_test_dataset = tokenized_test_dataset.rename_column(\"label\", \"labels\")\n",
        "\n",
        "# Set the format to PyTorch tensors\n",
        "tokenized_train_dataset.set_format(\"torch\")\n",
        "tokenized_val_dataset.set_format(\"torch\")\n",
        "tokenized_test_dataset.set_format(\"torch\")\n",
        "\n",
        "print(\"\\nProcessed dataset sample (train):\")\n",
        "print(tokenized_train_dataset[0])"
      ],
      "metadata": {
        "id": "7nhLaqrXzR0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1) # Get the index of the highest logit\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    f1 = f1_score(labels, predictions, average='binary') # Use 'weighted' for multi-class\n",
        "    return {\"accuracy\": accuracy, \"f1\": f1}"
      ],
      "metadata": {
        "id": "PBiWay5rzcgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the directory where model checkpoints will be saved\n",
        "output_dir = \"./sentiment_model_results\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,                   # Directory to save model checkpoints\n",
        "    num_train_epochs=3,                      # Total number of training epochs (start with 1-3)\n",
        "    per_device_train_batch_size=16,          # Batch size per device during training\n",
        "    per_device_eval_batch_size=32,           # Batch size for evaluation\n",
        "    warmup_steps=500,                        # Number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,                       # Strength of weight decay regularization\n",
        "    logging_dir='./logs',                    # Directory for storing logs\n",
        "    logging_steps=100,                       # Log metrics every N steps\n",
        "    evaluation_strategy=\"epoch\",             # Evaluate performance at the end of each epoch\n",
        "    save_strategy=\"epoch\",                   # Save a model checkpoint at the end of each epoch\n",
        "    load_best_model_at_end=True,             # Load the best model (based on validation metric) at the end\n",
        "    metric_for_best_model=\"f1\",              # Metric to determine the best model (can be accuracy, f1, etc.)\n",
        "    greater_is_better=True,                  # True if a higher metric value is better\n",
        "    fp16=torch.cuda.is_available(),          # Use mixed precision training if GPU is available (faster, less memory)\n",
        "    report_to=\"none\"                         # Disable reporting to external services like W&B for this example\n",
        ")"
      ],
      "metadata": {
        "id": "46rg8D_dz72Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,                             # The instantiated Transformers model to be trained\n",
        "    args=training_args,                      # Training arguments defined above\n",
        "    train_dataset=tokenized_train_dataset,   # Training dataset\n",
        "    eval_dataset=tokenized_val_dataset,      # Evaluation dataset\n",
        "    tokenizer=tokenizer,                     # Tokenizer (needed for padding collation)\n",
        "    compute_metrics=compute_metrics          # Function to compute evaluation metrics\n",
        ")"
      ],
      "metadata": {
        "id": "mZKGg1NN0KGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nStarting training...\")\n",
        "train_result = trainer.train()\n",
        "print(\"\\nTraining finished.\")\n",
        "\n",
        "# You can print some training stats\n",
        "print(f\"Training Metrics: {train_result.metrics}\")"
      ],
      "metadata": {
        "id": "qCKpTki20PBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nEvaluating on the test set...\")\n",
        "eval_results = trainer.evaluate(eval_dataset=tokenized_test_dataset)\n",
        "\n",
        "print(\"\\nTest Set Evaluation Results:\")\n",
        "print(eval_results)\n",
        "# Example output: {'eval_loss': 0.XXXX, 'eval_accuracy': 0.YYYY, 'eval_f1': 0.ZZZZ, ...}"
      ],
      "metadata": {
        "id": "rhd-3LT70Sjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Step 8: Save the Fine-Tuned Model and Tokenizer ===\n",
        "\n",
        "print(\"\\n--- Saving the model ---\")\n",
        "\n",
        "# Define the directory where you want to save the final model\n",
        "final_model_dir = \"./final_sentiment_model\"\n",
        "\n",
        "# Save the model using the Trainer's save_model function\n",
        "# This saves the model weights and configuration file config.json\n",
        "trainer.save_model(final_model_dir)\n",
        "print(f\"Model saved to {final_model_dir}\")\n",
        "\n",
        "# Explicitly save the tokenizer configuration and vocabulary\n",
        "# This ensures all necessary tokenizer files are in the same directory\n",
        "tokenizer.save_pretrained(final_model_dir)\n",
        "print(f\"Tokenizer saved to {final_model_dir}\")\n"
      ],
      "metadata": {
        "id": "woQ-znWM36l5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Step 9: Load Saved Model and Make Predictions ===\n",
        "# Simulate loading the model as if in a new session or script\n",
        "\n",
        "print(\"\\n--- Loading the saved model and tokenizer ---\")\n",
        "\n",
        "# Make sure necessary classes are imported (usually done at the top of notebook)\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Define the directory containing the saved model and tokenizer\n",
        "saved_model_dir = \"./final_sentiment_model\" # Should be the same as final_model_dir above\n",
        "\n",
        "# Load the tokenizer\n",
        "loaded_tokenizer = AutoTokenizer.from_pretrained(saved_model_dir)\n",
        "print(\"Tokenizer loaded successfully.\")\n",
        "\n",
        "# Load the model\n",
        "loaded_model = AutoModelForSequenceClassification.from_pretrained(saved_model_dir)\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "# Determine device (GPU or CPU) and move the loaded model to it\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "loaded_model.to(device)\n",
        "print(f\"Loaded model moved to device: {device}\")\n",
        "\n",
        "# --- Define Prediction Function using the Loaded Model ---\n",
        "\n",
        "def predict_sentiment_loaded_model(text):\n",
        "    # Set the loaded model to evaluation mode (disables dropout, etc.)\n",
        "    loaded_model.eval()\n",
        "\n",
        "    # Tokenize the input text using the loaded tokenizer\n",
        "    # return_tensors='pt' specifies PyTorch tensors\n",
        "    # max_length ensures consistency if needed, though truncation handles long inputs\n",
        "    inputs = loaded_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "\n",
        "    # Move tokenized inputs to the same device as the model\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Perform inference without calculating gradients\n",
        "    with torch.no_grad():\n",
        "        outputs = loaded_model(**inputs)\n",
        "\n",
        "    # Get the raw output scores (logits)\n",
        "    logits = outputs.logits\n",
        "\n",
        "    # Convert logits to probabilities using softmax\n",
        "    probabilities = torch.softmax(logits, dim=-1)\n",
        "\n",
        "    # Get the predicted class index (0 or 1) by finding the index with the highest probability\n",
        "    predicted_class_idx = torch.argmax(probabilities, dim=-1).item()\n",
        "\n",
        "    # Map index to human-readable label (ensure this map matches your training labels)\n",
        "    label_map = {0: \"Negative\", 1: \"Positive\"}\n",
        "    predicted_label = label_map[predicted_class_idx]\n",
        "\n",
        "    # Get the confidence score (probability of the predicted class)\n",
        "    confidence_score = probabilities[0][predicted_class_idx].item()\n",
        "\n",
        "    return {\"label\": predicted_label, \"confidence\": f\"{confidence_score:.4f}\"}\n",
        "\n",
        "\n",
        "# --- Test Predictions with the Loaded Model ---\n",
        "\n",
        "print(\"\\n--- Making predictions with the loaded model ---\")\n",
        "\n",
        "review1 = \"I absolutely loved this movie! The acting was incredible and the story was captivating.\"\n",
        "review2 = \"This was a complete waste of my time and money. Terrible plot, bad acting.\"\n",
        "review3 = \"It was an okay movie, not great but not terrible either. Kind of neutral.\" # May challenge binary classification\n",
        "\n",
        "print(f\"Review: '{review1}'\")\n",
        "prediction1 = predict_sentiment_loaded_model(review1)\n",
        "print(f\"Prediction: {prediction1}\")\n",
        "\n",
        "print(f\"\\nReview: '{review2}'\")\n",
        "prediction2 = predict_sentiment_loaded_model(review2)\n",
        "print(f\"Prediction: {prediction2}\")\n",
        "\n",
        "print(f\"\\nReview: '{review3}'\")\n",
        "prediction3 = predict_sentiment_loaded_model(review3)\n",
        "print(f\"Prediction: {prediction3}\") # Output might be Positive or Negative with lower confidence\n"
      ],
      "metadata": {
        "id": "jd_uVAZZDYBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h72ddS3_DacQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
